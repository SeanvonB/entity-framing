{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq3nkqZSJ_QF"
      },
      "outputs": [],
      "source": [
        "# Setup notebook\n",
        "!pip install datasets spacy spacy_experimental\n",
        "!pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.1/en_coreference_web_trf-3.4.0a2-py3-none-any.whl\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import spacy_experimental\n",
        "\n",
        "from copy import deepcopy\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation Source: https://github.com/explosion/spaCy/discussions/11585"
      ],
      "metadata": {
        "id": "_VZGTxUlAp2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process provided training data\n",
        "def collate_documents(path: str, langs: list = [\"EN\"]) -> None:\n",
        "    \"\"\"\n",
        "    Collate annotations with raw documents and convert to CSV.\n",
        "\n",
        "    Args:\n",
        "        path: Path to semeval_train folder with provided file structure\n",
        "        langs: List of language folders to process\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    path = path.rstrip(\"/\")\n",
        "    header = [\n",
        "        \"document\", \"mention\", \"start\", \"end\",\n",
        "        \"superlabel\", \"label1\", \"label2\", \"label3\"\n",
        "    ]\n",
        "\n",
        "    for lang in langs:\n",
        "        filename = f\"{path}/{lang}/subtask-1-annotations.txt\"\n",
        "        df = pd.read_csv(filename, sep=\"\\t\", header=None, names=header)\n",
        "        texts = []\n",
        "\n",
        "        for document in df[\"document\"]:\n",
        "            with open(f\"{path}/{lang}/raw-documents/{document}\", \"r\") as f:\n",
        "                texts.append(f.read())\n",
        "\n",
        "        df.insert(1, \"text\", texts, True)\n",
        "        df.to_csv(f\"{path}/subtask1_{lang}_clean.csv\", index=False)\n",
        "\n",
        "# collate_documents(\"/content/drive/MyDrive/semeval_train\")"
      ],
      "metadata": {
        "id": "en-fQmNzb08I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratify samples and create train/test split\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/semeval_train/subtask1_EN_clean.csv\")[\"train\"]\n",
        "dataset = dataset.add_column(\"stratify\", deepcopy(dataset[\"label1\"]))\n",
        "dataset = dataset.class_encode_column(\"stratify\")\n",
        "dataset = dataset.train_test_split(\n",
        "    test_size=0.2,\n",
        "    stratify_by_column=\"stratify\"\n",
        ")\n",
        "dataset = dataset.remove_columns(\"stratify\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "veDStE_JK5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup coreference resolver\n",
        "nlp = spacy.load(\"en_coreference_web_trf\")\n",
        "\n",
        "def resolve(entity: str, text: str) -> str:\n",
        "    \"\"\"\n",
        "    Resolve all coreferences of entity in provided text.\n",
        "\n",
        "    Args:\n",
        "        entity: Entity to resolve coreferences for\n",
        "        text: Text to resolve coreferences in\n",
        "\n",
        "    Returns:\n",
        "        Text with all coreferences of entity resolved\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    clusters = [v for k, v in doc.spans.items() if k.startswith(\"coref\")]\n",
        "    mapper = {}\n",
        "    output = \"\"\n",
        "\n",
        "    # Iterate through each coreference Span in SpanGroup\n",
        "    for cluster in clusters:\n",
        "\n",
        "        # Skip clusters without entity\n",
        "        if entity not in [mention.text for mention in cluster]:\n",
        "            continue\n",
        "\n",
        "        # Map all other mentions of entity\n",
        "        for mention in cluster:\n",
        "            if mention.text == entity:\n",
        "                continue\n",
        "\n",
        "            # Replace first token of span with entire entity span\n",
        "            mapper[mention[0].idx] = entity + mention[0].whitespace_\n",
        "\n",
        "            # Erase other tokens in span\n",
        "            for token in mention[1:]:\n",
        "                mapper[token.idx] = \"\"\n",
        "\n",
        "    # Build output by substituting mapper spans for tokens\n",
        "    for token in doc:\n",
        "        if token.idx in mapper:\n",
        "            output += mapper[token.idx]\n",
        "        else:\n",
        "            output += token.text + token.whitespace_\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "BqL1U6lymfxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply coreference resolution to copy of test split\n",
        "def apply_resolve(sample):\n",
        "    sample[\"text\"] = resolve(sample[\"mention\"], sample[\"text\"])\n",
        "\n",
        "    return sample\n",
        "\n",
        "coref_dataset = deepcopy(dataset[\"test\"])\n",
        "coref_dataset = coref_dataset.map(apply_resolve)\n",
        "coref_dataset"
      ],
      "metadata": {
        "id": "p67gV9BlQFyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new coref split to dataset and save\n",
        "dataset[\"coref\"] = coref_dataset\n",
        "dataset.save_to_disk(\"/content/drive/MyDrive/semeval_splits\")\n",
        "foo = load_from_disk(\"/content/drive/MyDrive/semeval_splits\")"
      ],
      "metadata": {
        "id": "BVf5Th-NVWU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}